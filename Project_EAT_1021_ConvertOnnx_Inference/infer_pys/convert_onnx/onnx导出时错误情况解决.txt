pt转为onnx的时候，会出现一些问题比如

1. 自适应池化  adaptive_pool2d 不应该被使用，而是直接固定kernel_size
2. 一些卷积核（自适应动态卷积）不该被使用，而是直接固定卷积核的大小


如下是要修改的代码，修改后可以通过代码-------------- 正常导出为onnx（已经得到验证，没有偏差）
G:\SYZ_Projects\Audio_Class_SYZ\Project_EAT\convert_onnx\pt2onnx.py

修改1
models.dymn.models.py
        elif self.head_type == "mlp":
            self.classifier = nn.Sequential(
                # nn.AdaptiveAvgPool2d(1),##onnx error need to convert  AvgPool2d
                nn.AvgPool2d(kernel_size=(4,7)),
                nn.Flatten(start_dim=1),
                nn.Linear(lastconv_output_channels, last_channel),
                nn.Hardswish(inplace=True),
                nn.Dropout(p=dropout, inplace=True),
                nn.Linear(last_channel, num_classes),
            )

修改2
models.dymn.models.py
    def _clf_forward(self, x: Tensor):
        # x1=x ## shape==1,1920,4,7
        # embed = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1) ##shape==1,1920
        # embed = F.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1) ##onnx error need to convert  F.avg_pool2d
        embed = F.avg_pool2d(input=x, kernel_size=(4, 7)).view(x.size(0), -1)
        # x1=x ###time ===2s ---> 1,128,200----XXXX--->1,1920,4,7
        # x2 = self.classifier_before(x1)  #1,1920,1,1
        # x = self.classifier_after(x2).squeeze()
        # x3 = embed
        x = self.classifier(x).squeeze()
        # x = self.classifier(x).squeeze()
        if x.dim() == 1:
            # squeezed batch dimension
            x = x.unsqueeze(0)
        return x, embed


修改3
models.dymn.dy_block.py
    def forward(self, x, g):
        ##x.shape==1,32,64,100
        # cf = F.adaptive_avg_pool2d(x, (None, 1))
        # ct = F.adaptive_avg_pool2d(x, (1, None)).permute(0, 1, 3, 2)
        # onnx convert error   need to  convert  avg_pool2d
        H = x.shape[2]
        W = x.shape[3]
        cf = F.avg_pool2d(x, kernel_size=(1, int(W)))
        ct = F.avg_pool2d(x, kernel_size=(int(H), 1)).permute(0, 1, 3, 2)
        # cf = F.avg_pool2d(x, kernel_size=(1, 100))  # [B,C,H,1]
        # ct = F.avg_pool2d(x, kernel_size=(64,1)).permute(0, 1, 3, 2)  # [B,C,W,1]
        # f, t = cf.size(2), ct.size(2)

        f, t = cf.size(2), ct.size(2)


修改4
models.dymn.dy_block.py
    def forward(self, x, g=None):
        b, c, f, t = x.size()
        g_c = g[0].view(b, -1)
        residuals = self.residuals(g_c).view(b, self.att_groups, 1, -1)
        attention = F.softmax(residuals / self.temperature, dim=-1)

        # attention shape: batch_size x 1 x 1 x k
        # self.weight shape: 1 x 1 x k x out_channels * (in_channels // groups) * kernel_size ** 2
        aggregate_weight = (attention @ self.weight).transpose(1, 2).reshape(b, self.out_channels,
                                                                             self.in_channels // self.groups,
                                                                             self.kernel_size, self.kernel_size)

        # aggregate_weight shape: batch_size x out_channels x in_channels // groups x kernel_size x kernel_size
        # t1=aggregate_weight
        aggregate_weight = aggregate_weight.view(self.out_channels, self.in_channels // self.groups,
                                                 self.kernel_size, self.kernel_size)
        # t3=aggregate_weight
        # each sample in the batch has different weights for the convolution - therefore batch and channel dims need to
        # be merged together in channel dimension
        x = x.view(1, -1, f, t)